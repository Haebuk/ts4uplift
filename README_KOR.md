# Backend-TS Dataset

Official repository of the paper: **Modeling Uplift from Observational Time-Series in Continual Scenarios** (Oral presentation, AAAI 2023 Bridge Program for Continual Causality)

[https://openreview.net/forum?id=pKyB5wMnTiy](https://openreview.net/forum?id=pKyB5wMnTiy)


## 초록

기계학습에서 인과관계의 중요성이 증가함에 따라, 모델이 분포의 변화에도 견고하도록 정확한 원인기제를 학습할 수 있기를 기대합니다. 기존 벤치마크들이 시각 및 자연어 분야에 집중된 반면, 인과관계 추론 문제에서의 도메인 및 시간적 변화는 충분히 조사되지 않았습니다. 이를 위해 우리는 지속 학습 시나리오에서의 업리프트를 모델링하기 위한 **Backend-TS** 데이터셋을 소개합니다. 우리는 CRUD 데이터를 이용해 데이터셋을 구축했으며, 시간적 및 도메인 변화 하에서 지속 학습 테스크들을 제안합니다.


## 소개

본 데이터셋은 [AFI Inc.](https://www.afidev.com/)와의 협업을 통해 개발되었습니다. AFI Inc.는 모바일 게임을 위한 백엔드 서비스를 제공하는 기업으로, [thebackend.io](https://www.thebackend.io/)라는 서비스를 통해 백엔드 서버 로그를 수집하고 있습니다. 2023년 1월 현재, 이 서비스는 3,174개의 모바일 게임에 사용되고 있으며, 누적 유저 수는 60백만 명을 넘어섰으며, 하루에 115백만 건의 트랜잭션이 발생하고 있습니다. 본 데이터셋은 이러한 로그 데이터를 활용하여 유저의 행동을 예측하여, 게임 서비스의 운영을 개선하는데에 목적이 있습니다.

대부분 인터넷 서비스의 로그 정보는 매일 거대한 규모로 생성되지만, 그 규모에 비해 그 유효성은 미비합니다. 따라서 이러한 로그 데이터는 일반적으로 데이터 웨어하우스에 저장되며, 데이터 과학 및 분석에 사용되는 비율은 매우 적습니다. 이때 분석은 서비스와 회사 수준의 거시적 관점에서 수행되지만, 개별 사용자의 관심사나 예측은 상대적으로 덜 중요합니다. 그러나 이 데이터셋의 관심사는 개별 사용자의 행동을 예측하는 것으로, 사용자 만족도와 서비스 품질을 향상시키는 데에 기여합니다. 또한 기계 학습 및 데이터 마이닝의 관점에서, 데이터셋은 최소한의 전처리를 통해 만들어져 사람에 의한 특징 공학 및 선택에 대한 의존을 줄이고 더 많은 정보를 포함하도록 개발되었습니다.


## 태스크 설명

태스크는 크게 ID (In-distribution), TS (Temporal Shift), OOD (Out-of-domain)으로 구성되어 있고, 업리프트 모델링에 사용되는 게임은 총 3개(A, B, C)입니다. 이를 정리하면 아래의 표와 같습니다. 

|   Task   |           Train set           |          Valid set           |  Test set  |
|:--------:|:------------------------------|:-----------------------------|:----------:|
|    ID    | Game A APR + MAY              | Game A APR + MAY (20% split) |      -     |
|    TS    | Game A APR + MAY              | Game A APR + MAY (20% split) | Game A JUN |
|  OOD w/  | Game A APR + MAY & Game B JUN | Game B JUN (20% split)       | Game B JUL |
|  OOD w/o | Game A APR + MAY              | Game A APR + MAY (20% split) | Game C JUL |

* ID는 게임 A의 전체 데이터에서 임의로 추출된 검증 데이터(validation set)에서의 성능을 측정하는 것입니다. 나머지는 훈련 데이터(train set)으로 사용합니다. 훈련 데이터와 검증 데이터가 동일한 게임과 시점에서 수집된 데이터이기 때문에 분포가 동일하다고 가정할 수 있고, 이는 업리프트 모델의 베이스라인 성능 측정을 위해서 사용됩니다.

* TS는 ID에서 훈련된 모델의 성능을 훈련 데이터로부터 한 달 가량 떨어진 시점에서 수집된 평가 데이터(test set)로 성능을 평가합니다. 이는 업리프트 모델이 과거의 데이터로 훈련 시킨 모델을 기반으로 미래 사용자의 행동을 예측한다는 점에서 ID보다 더 현실적인 시나리오입니다. 만약 모델이 시간축에 의한 분포 변화에 견고한 특성들을 배웠다면, TS에서의 성능은 ID와 유사할 것입니다.

* OOD는 위에서 훈련된 모델에서 사용된 것과 다른 게임인 B와 C에서 수집된 평가 데이터(test set)로 성능을 평가합니다. 이는 업리프트 모델이 훈련 데이터와는 다른 게임의 사용자의 행동을 예측한다는 점에서 모델의 더 넓은 범위의 일반화 성능을 평가합니다. OOD는 2개의 세부적인 태스크로 구분됩니다. 

    * OOD w/: 게임 B의 새로운 학습 데이터가 주어지며, 위 ID에서 훈련한 모델을 새로운 게임에 파인튜닝(fine-tuning)한 후, 성능을 학습 데이터보다 미래 시점의 평가 데이터에서 평가합니다. 이는 도메인이 변동되었을 때 모델의 일반화 성능을 평가합니다.

    * OOD w/o: 게임 C에서 수집된 평가 데이터를 사용하여 ID에서 훈련된 모델의 성능을 평가합니다. 마찬가지로 이는 도메인이 변동되었을 때 모델의 일반화 성능을 평가한다는 점에서 동일하지만, ID에서 훈련된 모델에 대한 미세조정(fine-tuning)을 수행하지 않습니다. 만약 모델이 도메인 변동에 견고한 특성들을 배웠다면, OOD w/o에서의 성능은 준수한 수준일 것입니다.
